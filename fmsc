import numpy as np
from scipy.sparse import csr_matrix
from scipy.sparse.linalg import svds
from sklearn.cluster import KMeans
from sklearn.metrics import pairwise_distances
from sklearn.preprocessing import normalize
import matplotlib.pyplot as plt
import seaborn as sns


# 生成随机多视图数据
def generate_random_multiview_data(num_samples, num_features, num_views):
    data = []
    for _ in range(num_views):
        view = np.random.rand(num_samples, num_features) * np.random.rand(num_features)  # 增加多样性
        data.append(normalize(view, axis=0))  # 归一化处理
    return data


# 计算图拉普拉斯矩阵
def compute_graph_laplacian(data_view):
    affinity_matrix = np.exp(-pairwise_distances(data_view, metric='sqeuclidean') / (2. * (data_view.std() ** 2)))
    degree_matrix = np.diag(affinity_matrix.sum(axis=1))
    laplacian_matrix = degree_matrix - affinity_matrix
    return csr_matrix(laplacian_matrix)


# 计算公平感知正则化项
def compute_fairness_aware_regularization(Y, P, epsilon=1e-10):
    YTY = Y.T @ Y
    YTY_inv = np.linalg.inv(YTY + epsilon * np.eye(YTY.shape[0]))  # 添加正则化项
    PTP_YTY_inv = P.T @ Y @ YTY_inv @ Y.T @ P
    trace_term = np.trace(PTP_YTY_inv)
    return trace_term


# 计算相对于Y的梯度
def compute_gradient_Y(Y, L_combined, fairness_term):
    # 计算目标函数的梯度
    # 这里需要根据具体的目标函数推导出梯度
    # 下面是一个示例，实际需要根据论文的公式进行推导
    grad_Y = (L_combined @ Y) + fairness_term  # 这是一个占位符
    return grad_Y


# 计算相对于alpha的梯度
def compute_gradient_alpha(Y, alpha, L_views):
    # 这里需要根据具体的目标函数推导出梯度
    # 下面是一个示例，实际需要根据论文的公式进行推导
    grad_alpha = np.zeros_like(alpha)  # 这是一个占位符
    return grad_alpha


# 优化算法（这里使用梯度下降法）
def optimize_fmsc(data, num_clusters, lambda_fair, max_iter=100, learning_rate=0.01):
    num_samples, num_features = data[0].shape
    num_views = len(data)
    # 初始化聚类指示矩阵Y（随机分配样本到簇中）
    Y = np.random.randint(0, 2, (num_samples, num_clusters))
    Y = normalize(Y, norm='l1', axis=1)  # 确保每行和为1
    # 初始化视图权重α（均匀分配）
    alpha = np.ones(num_views) / num_views
    # 构造保护组矩阵P
    num_protected_groups = 2  # 假设有两个保护组
    P = np.random.randint(0, 2, (num_samples, num_protected_groups))

    for _ in range(max_iter):
        # 计算每个视图的拉普拉斯矩阵
        L_views = [compute_graph_laplacian(view) for view in data]
        # 计算目标函数中的拉普拉斯部分
        L_combined = sum(alpha[i] * L_views[i].toarray() for i in range(num_views))

        # 计算目标函数的跟踪部分
        YTY = Y.T @ Y
        tr_YLY = np.trace(Y.T @ L_combined @ Y) / (np.trace(YTY))

        # 计算公平感知正则化项
        fairness_term = compute_fairness_aware_regularization(Y, P)

        # 计算目标函数梯度
        grad_Y = compute_gradient_Y(Y, L_combined, fairness_term)  # 需要实现该函数
        grad_alpha = compute_gradient_alpha(Y, alpha, L_views)  # 需要实现该函数

        # 更新Y和alpha
        Y -= learning_rate * grad_Y
        alpha -= learning_rate * grad_alpha

        # 归一化Y和alpha
        Y = normalize(Y, norm='l1', axis=1)
        alpha = alpha / np.sum(alpha)  # 确保α的和为1

    # 使用KMeans对最终的Y进行离散化得到聚类结果
    kmeans = KMeans(n_clusters=num_clusters)
    cluster_labels = kmeans.fit_predict(Y)

    return cluster_labels, alpha


# 设置参数
num_samples = 100
num_features = 50
num_views = 4
num_clusters = 2
lambda_fair = 1.0

# 生成随机多视图数据
data = generate_random_multiview_data(num_samples, num_features, num_views)

# 优化FMSC方法
cluster_labels, alpha = optimize_fmsc(data, num_clusters, lambda_fair)

# 输出聚类结果和视图权重
print("Cluster Labels:", cluster_labels)
print("View Weights:", alpha)

# 可视化聚类结果
plt.figure(figsize=(10, 6))
sns.scatterplot(x=data[0][:, 0], y=data[0][:, 1], hue=cluster_labels, palette='viridis')
plt.title('Cluster Visualization')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend(title='Cluster')
plt.show()

# 可视化视图权重
plt.figure(figsize=(6, 4))
sns.barplot(x=np.arange(len(alpha)), y=alpha)
plt.title('View Weights')
plt.xlabel('View Index')
plt.ylabel('Weight')
plt.show()
